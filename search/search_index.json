{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome aboard!","title":"Welcome aboard!"},{"location":"#welcome-aboard","text":"","title":"Welcome aboard!"},{"location":"automatic-cleanup/","text":"Automatic Cleanup This document explains the automatic PVC cleanup feature that removes cloned PVCs after successful backup operations, helping to manage storage resources efficiently. Overview The automatic cleanup feature allows the DataMover Operator to automatically delete cloned PVCs after successful data synchronization. This helps prevent storage waste, reduces operational overhead, and maintains a clean Kubernetes environment. Feature Configuration Enabling Automatic Cleanup Enable automatic cleanup in your DataMover specification: apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: auto-cleanup-backup spec: sourcePvc: \"app-data\" secretName: \"storage-credentials\" deletePvcAfterBackup: true # Enable automatic cleanup Disabling Automatic Cleanup Keep cloned PVCs for manual management: apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: manual-cleanup-backup spec: sourcePvc: \"app-data\" secretName: \"storage-credentials\" deletePvcAfterBackup: false # Disable automatic cleanup Default Behavior : deletePvcAfterBackup: false Cleanup Workflow Phase Progression When automatic cleanup is enabled, the DataMover follows this workflow: CreatingClonedPVC : Create clone of source PVC ClonedPVCReady : Clone is bound and ready CreatingPod : Execute rclone job for data sync CleaningUp : Delete cloned PVC (if deletePvcAfterBackup: true ) Completed : Operation finished successfully Phase Progression Without Cleanup When automatic cleanup is disabled: CreatingClonedPVC : Create clone of source PVC ClonedPVCReady : Clone is bound and ready CreatingPod : Execute rclone job for data sync Completed : Operation finished (clone PVC remains) Cleanup Trigger Cleanup is triggered only after: \u2705 Successful job completion : Rclone job completes successfully \u2705 Data sync verification : Sync operation reports success \u2705 Status confirmation : Job status shows completion Cleanup is NOT triggered when: \u274c Job fails : Any failure prevents cleanup \u274c Sync errors : Data synchronization errors prevent cleanup \u274c Operator errors : Internal operator errors prevent cleanup Implementation Details Cleanup Logic The cleanup process involves: // Simplified cleanup logic func (r *DataMoverReconciler) cleanupClonedPVC(ctx context.Context, dm *datamoverv1alpha1.DataMover) { if dm.Status.RestoredPVCName == \"\" { // No PVC to cleanup return } // Delete the cloned PVC pvc := &corev1.PersistentVolumeClaim{ ObjectMeta: metav1.ObjectMeta{ Name: dm.Status.RestoredPVCName, Namespace: dm.Namespace, }, } if err := r.Delete(ctx, pvc); err != nil { // Handle deletion error return err } // Record cleanup metrics metrics.RecordCleanupOperation(\"success\", dm.Namespace) } Error Handling If cleanup fails: Retry : Cleanup is retried on subsequent reconciliation Logging : Failure is logged with detailed error information Metrics : Cleanup failure is recorded in metrics Status : DataMover phase remains \"CleaningUp\" until successful Safety Mechanisms The cleanup process includes safety checks: PVC Ownership : Only delete PVCs created by the operator Status Verification : Confirm successful job completion before cleanup Error Recovery : Handle partial cleanup scenarios gracefully Benefits Storage Management Automatic cleanup provides: Cost Reduction : Eliminates storage costs for temporary clones Resource Efficiency : Prevents storage quota exhaustion Clean Environment : Maintains organized Kubernetes resources Operational Benefits Reduced Manual Work : No need for manual PVC cleanup Consistent Behavior : Predictable resource lifecycle Automation : Fits well into automated backup workflows Example Storage Savings Consider a backup operation for a 100GB PVC: Without Cleanup : Original PVC: 100GB (permanent) Clone PVC: 100GB (remains after backup) Total Usage: 200GB With Cleanup : Original PVC: 100GB (permanent) Clone PVC: 100GB (deleted after backup) Total Usage: 100GB after completion Savings : 50% storage reduction per backup operation Monitoring Cleanup Operations Metrics The operator provides Prometheus metrics for cleanup operations: # Cleanup operation counters datamover_cleanup_operations_total{status=\"success\", namespace=\"default\"} datamover_cleanup_operations_total{status=\"failure\", namespace=\"default\"} # Phase duration including cleanup datamover_phase_duration_seconds{phase=\"CleaningUp\", namespace=\"default\"} Status Tracking Monitor cleanup through DataMover status: # Watch cleanup progress kubectl get datamover my-backup -w # Check detailed status kubectl describe datamover my-backup Expected output during cleanup: status: phase: \"CleaningUp\" restoredPvcName: \"restored-app-data-20240806143052\" Logging Monitor cleanup operations through operator logs: # View cleanup logs kubectl logs -n datamover-operator-system deployment/datamover-operator-controller-manager | grep -i cleanup # Example log entries # INFO Cleaning up cloned PVC {\"pvc\": \"restored-app-data-20240806143052\"} # INFO Successfully deleted cloned PVC {\"pvc\": \"restored-app-data-20240806143052\"} Use Cases 1. Automated Backup Workflows Perfect for scheduled backups where clones are temporary: apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: nightly-backup spec: sourcePvc: \"production-data\" secretName: \"backup-credentials\" deletePvcAfterBackup: true addTimestampPrefix: true Workflow : 1. Clone production PVC 2. Sync to timestamped backup location 3. Automatically delete clone 4. Preserve only original PVC 2. Development Environment Snapshots For development workflows where clones are not needed after sync: apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: dev-snapshot spec: sourcePvc: \"dev-workspace\" secretName: \"dev-storage\" deletePvcAfterBackup: true 3. Compliance Backups For compliance where only the backup copy matters: apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: compliance-backup spec: sourcePvc: \"financial-records\" secretName: \"compliance-storage\" deletePvcAfterBackup: true addTimestampPrefix: true When NOT to Use Cleanup 1. Clone Analysis Workflows When you need to analyze or compare cloned data: apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: data-analysis spec: sourcePvc: \"production-data\" secretName: \"storage-credentials\" deletePvcAfterBackup: false # Keep clone for analysis 2. Multi-Stage Backups When clones are used in multiple backup stages: # First stage: Create clone and initial backup apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: stage1-backup spec: sourcePvc: \"app-data\" secretName: \"primary-storage\" deletePvcAfterBackup: false # Keep for stage 2 # Second stage: Use same clone for secondary backup # (would reference the same cloned PVC) 3. Debugging Scenarios When troubleshooting backup issues: apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: debug-backup spec: sourcePvc: \"problematic-data\" secretName: \"storage-credentials\" deletePvcAfterBackup: false # Keep clone for debugging Troubleshooting Common Issues 1. Cleanup Stuck in Progress Symptoms : DataMover phase remains \"CleaningUp\" Possible Causes : - PVC has active pod attachments - PVC finalizers preventing deletion - RBAC permission issues Diagnosis : # Check PVC status kubectl get pvc <cloned-pvc-name> # Check for attached pods kubectl get pods --all-namespaces -o wide | grep <cloned-pvc-name> # Check PVC finalizers kubectl get pvc <cloned-pvc-name> -o yaml | grep finalizers # Check operator permissions kubectl auth can-i delete persistentvolumeclaims --as=system:serviceaccount:datamover-operator-system:datamover-operator-controller-manager 2. Cleanup Fails After Successful Sync Symptoms : Job succeeds but cleanup fails Possible Causes : - PVC in use by other processes - Storage class deletion policies - Volume attachment issues Solutions : # Force PVC deletion (if safe) kubectl patch pvc <cloned-pvc-name> -p '{\"metadata\":{\"finalizers\":[]}}' --type=merge # Check for volume attachments kubectl get volumeattachment | grep <pv-name> 3. Metrics Not Recording Cleanup Symptoms : Cleanup happens but metrics not updated Diagnosis : # Check operator logs for metric errors kubectl logs -n datamover-operator-system deployment/datamover-operator-controller-manager | grep -i metric # Verify Prometheus scraping curl http://operator-metrics-service:8080/metrics | grep cleanup Debug Commands # Monitor cleanup process kubectl get datamover <name> -w # Check cleanup logs kubectl logs -n datamover-operator-system deployment/datamover-operator-controller-manager | grep -i \"cleanup\\|delete\" # List PVCs created by operator kubectl get pvc -l app.kubernetes.io/created-by=datamover-operator # Check PVC deletion events kubectl get events --field-selector involvedObject.kind=PersistentVolumeClaim Best Practices 1. Resource Planning Consider cleanup in resource planning: Temporary Storage : Plan for peak usage during clone creation Cleanup Timing : Consider cleanup duration in scheduling Quota Management : Account for temporary storage quota usage 2. Monitoring Set up monitoring for cleanup operations: # Example Prometheus alert groups: - name: datamover.cleanup rules: - alert: DataMoverCleanupFailing expr: increase(datamover_cleanup_operations_total{status=\"failure\"}[5m]) > 0 for: 0m labels: severity: warning annotations: summary: \"DataMover cleanup operations are failing\" description: \"DataMover cleanup failures in namespace {{ $labels.namespace }}\" 3. Backup Verification Always verify backup success before cleanup: # Verify backup exists before cleanup completes rclone lsd s3:my-bucket/ | grep $(date +%Y-%m-%d) 4. Testing Test cleanup behavior: # Test cleanup with small PVC apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: cleanup-test spec: sourcePvc: \"test-data\" secretName: \"test-credentials\" deletePvcAfterBackup: true 5. Documentation Document cleanup policies in your backup procedures: When cleanup is enabled/disabled Storage impact of cleanup decisions Recovery procedures if cleanup fails Advanced Scenarios Conditional Cleanup Implement conditional cleanup based on backup verification: # Example: Only cleanup after backup verification apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: verified-cleanup spec: sourcePvc: \"critical-data\" secretName: \"storage-credentials\" deletePvcAfterBackup: true additionalEnv: - name: \"VERIFY_BACKUP\" value: \"true\" Multi-Destination Cleanup When backing up to multiple destinations: # Primary backup with cleanup apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: primary-backup spec: sourcePvc: \"important-data\" secretName: \"primary-storage\" deletePvcAfterBackup: false # Keep for secondary backup # Secondary backup without cleanup apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: secondary-backup spec: sourcePvc: \"important-data\" # Same source secretName: \"secondary-storage\" deletePvcAfterBackup: true # Cleanup after both complete Cleanup with Lifecycle Management Integrate with external lifecycle management: #!/bin/bash # External cleanup verification script NAMESPACE=\"default\" DATAMOVER_NAME=\"my-backup\" # Wait for backup completion kubectl wait --for=condition=Complete datamover/$DATAMOVER_NAME -n $NAMESPACE --timeout=3600s # Verify backup in storage if rclone check s3:my-bucket/latest/ /verify/path/; then echo \"Backup verified, cleanup can proceed\" else echo \"Backup verification failed, manual intervention required\" exit 1 fi This comprehensive documentation covers all aspects of the automatic cleanup feature, providing users with the knowledge needed to effectively use and troubleshoot this functionality.","title":"Automatic Cleanup"},{"location":"automatic-cleanup/#automatic-cleanup","text":"This document explains the automatic PVC cleanup feature that removes cloned PVCs after successful backup operations, helping to manage storage resources efficiently.","title":"Automatic Cleanup"},{"location":"automatic-cleanup/#overview","text":"The automatic cleanup feature allows the DataMover Operator to automatically delete cloned PVCs after successful data synchronization. This helps prevent storage waste, reduces operational overhead, and maintains a clean Kubernetes environment.","title":"Overview"},{"location":"automatic-cleanup/#feature-configuration","text":"","title":"Feature Configuration"},{"location":"automatic-cleanup/#enabling-automatic-cleanup","text":"Enable automatic cleanup in your DataMover specification: apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: auto-cleanup-backup spec: sourcePvc: \"app-data\" secretName: \"storage-credentials\" deletePvcAfterBackup: true # Enable automatic cleanup","title":"Enabling Automatic Cleanup"},{"location":"automatic-cleanup/#disabling-automatic-cleanup","text":"Keep cloned PVCs for manual management: apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: manual-cleanup-backup spec: sourcePvc: \"app-data\" secretName: \"storage-credentials\" deletePvcAfterBackup: false # Disable automatic cleanup Default Behavior : deletePvcAfterBackup: false","title":"Disabling Automatic Cleanup"},{"location":"automatic-cleanup/#cleanup-workflow","text":"","title":"Cleanup Workflow"},{"location":"automatic-cleanup/#phase-progression","text":"When automatic cleanup is enabled, the DataMover follows this workflow: CreatingClonedPVC : Create clone of source PVC ClonedPVCReady : Clone is bound and ready CreatingPod : Execute rclone job for data sync CleaningUp : Delete cloned PVC (if deletePvcAfterBackup: true ) Completed : Operation finished successfully","title":"Phase Progression"},{"location":"automatic-cleanup/#phase-progression-without-cleanup","text":"When automatic cleanup is disabled: CreatingClonedPVC : Create clone of source PVC ClonedPVCReady : Clone is bound and ready CreatingPod : Execute rclone job for data sync Completed : Operation finished (clone PVC remains)","title":"Phase Progression Without Cleanup"},{"location":"automatic-cleanup/#cleanup-trigger","text":"Cleanup is triggered only after: \u2705 Successful job completion : Rclone job completes successfully \u2705 Data sync verification : Sync operation reports success \u2705 Status confirmation : Job status shows completion Cleanup is NOT triggered when: \u274c Job fails : Any failure prevents cleanup \u274c Sync errors : Data synchronization errors prevent cleanup \u274c Operator errors : Internal operator errors prevent cleanup","title":"Cleanup Trigger"},{"location":"automatic-cleanup/#implementation-details","text":"","title":"Implementation Details"},{"location":"automatic-cleanup/#cleanup-logic","text":"The cleanup process involves: // Simplified cleanup logic func (r *DataMoverReconciler) cleanupClonedPVC(ctx context.Context, dm *datamoverv1alpha1.DataMover) { if dm.Status.RestoredPVCName == \"\" { // No PVC to cleanup return } // Delete the cloned PVC pvc := &corev1.PersistentVolumeClaim{ ObjectMeta: metav1.ObjectMeta{ Name: dm.Status.RestoredPVCName, Namespace: dm.Namespace, }, } if err := r.Delete(ctx, pvc); err != nil { // Handle deletion error return err } // Record cleanup metrics metrics.RecordCleanupOperation(\"success\", dm.Namespace) }","title":"Cleanup Logic"},{"location":"automatic-cleanup/#error-handling","text":"If cleanup fails: Retry : Cleanup is retried on subsequent reconciliation Logging : Failure is logged with detailed error information Metrics : Cleanup failure is recorded in metrics Status : DataMover phase remains \"CleaningUp\" until successful","title":"Error Handling"},{"location":"automatic-cleanup/#safety-mechanisms","text":"The cleanup process includes safety checks: PVC Ownership : Only delete PVCs created by the operator Status Verification : Confirm successful job completion before cleanup Error Recovery : Handle partial cleanup scenarios gracefully","title":"Safety Mechanisms"},{"location":"automatic-cleanup/#benefits","text":"","title":"Benefits"},{"location":"automatic-cleanup/#storage-management","text":"Automatic cleanup provides: Cost Reduction : Eliminates storage costs for temporary clones Resource Efficiency : Prevents storage quota exhaustion Clean Environment : Maintains organized Kubernetes resources","title":"Storage Management"},{"location":"automatic-cleanup/#operational-benefits","text":"Reduced Manual Work : No need for manual PVC cleanup Consistent Behavior : Predictable resource lifecycle Automation : Fits well into automated backup workflows","title":"Operational Benefits"},{"location":"automatic-cleanup/#example-storage-savings","text":"Consider a backup operation for a 100GB PVC: Without Cleanup : Original PVC: 100GB (permanent) Clone PVC: 100GB (remains after backup) Total Usage: 200GB With Cleanup : Original PVC: 100GB (permanent) Clone PVC: 100GB (deleted after backup) Total Usage: 100GB after completion Savings : 50% storage reduction per backup operation","title":"Example Storage Savings"},{"location":"automatic-cleanup/#monitoring-cleanup-operations","text":"","title":"Monitoring Cleanup Operations"},{"location":"automatic-cleanup/#metrics","text":"The operator provides Prometheus metrics for cleanup operations: # Cleanup operation counters datamover_cleanup_operations_total{status=\"success\", namespace=\"default\"} datamover_cleanup_operations_total{status=\"failure\", namespace=\"default\"} # Phase duration including cleanup datamover_phase_duration_seconds{phase=\"CleaningUp\", namespace=\"default\"}","title":"Metrics"},{"location":"automatic-cleanup/#status-tracking","text":"Monitor cleanup through DataMover status: # Watch cleanup progress kubectl get datamover my-backup -w # Check detailed status kubectl describe datamover my-backup Expected output during cleanup: status: phase: \"CleaningUp\" restoredPvcName: \"restored-app-data-20240806143052\"","title":"Status Tracking"},{"location":"automatic-cleanup/#logging","text":"Monitor cleanup operations through operator logs: # View cleanup logs kubectl logs -n datamover-operator-system deployment/datamover-operator-controller-manager | grep -i cleanup # Example log entries # INFO Cleaning up cloned PVC {\"pvc\": \"restored-app-data-20240806143052\"} # INFO Successfully deleted cloned PVC {\"pvc\": \"restored-app-data-20240806143052\"}","title":"Logging"},{"location":"automatic-cleanup/#use-cases","text":"","title":"Use Cases"},{"location":"automatic-cleanup/#1-automated-backup-workflows","text":"Perfect for scheduled backups where clones are temporary: apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: nightly-backup spec: sourcePvc: \"production-data\" secretName: \"backup-credentials\" deletePvcAfterBackup: true addTimestampPrefix: true Workflow : 1. Clone production PVC 2. Sync to timestamped backup location 3. Automatically delete clone 4. Preserve only original PVC","title":"1. Automated Backup Workflows"},{"location":"automatic-cleanup/#2-development-environment-snapshots","text":"For development workflows where clones are not needed after sync: apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: dev-snapshot spec: sourcePvc: \"dev-workspace\" secretName: \"dev-storage\" deletePvcAfterBackup: true","title":"2. Development Environment Snapshots"},{"location":"automatic-cleanup/#3-compliance-backups","text":"For compliance where only the backup copy matters: apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: compliance-backup spec: sourcePvc: \"financial-records\" secretName: \"compliance-storage\" deletePvcAfterBackup: true addTimestampPrefix: true","title":"3. Compliance Backups"},{"location":"automatic-cleanup/#when-not-to-use-cleanup","text":"","title":"When NOT to Use Cleanup"},{"location":"automatic-cleanup/#1-clone-analysis-workflows","text":"When you need to analyze or compare cloned data: apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: data-analysis spec: sourcePvc: \"production-data\" secretName: \"storage-credentials\" deletePvcAfterBackup: false # Keep clone for analysis","title":"1. Clone Analysis Workflows"},{"location":"automatic-cleanup/#2-multi-stage-backups","text":"When clones are used in multiple backup stages: # First stage: Create clone and initial backup apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: stage1-backup spec: sourcePvc: \"app-data\" secretName: \"primary-storage\" deletePvcAfterBackup: false # Keep for stage 2 # Second stage: Use same clone for secondary backup # (would reference the same cloned PVC)","title":"2. Multi-Stage Backups"},{"location":"automatic-cleanup/#3-debugging-scenarios","text":"When troubleshooting backup issues: apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: debug-backup spec: sourcePvc: \"problematic-data\" secretName: \"storage-credentials\" deletePvcAfterBackup: false # Keep clone for debugging","title":"3. Debugging Scenarios"},{"location":"automatic-cleanup/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"automatic-cleanup/#common-issues","text":"","title":"Common Issues"},{"location":"automatic-cleanup/#1-cleanup-stuck-in-progress","text":"Symptoms : DataMover phase remains \"CleaningUp\" Possible Causes : - PVC has active pod attachments - PVC finalizers preventing deletion - RBAC permission issues Diagnosis : # Check PVC status kubectl get pvc <cloned-pvc-name> # Check for attached pods kubectl get pods --all-namespaces -o wide | grep <cloned-pvc-name> # Check PVC finalizers kubectl get pvc <cloned-pvc-name> -o yaml | grep finalizers # Check operator permissions kubectl auth can-i delete persistentvolumeclaims --as=system:serviceaccount:datamover-operator-system:datamover-operator-controller-manager","title":"1. Cleanup Stuck in Progress"},{"location":"automatic-cleanup/#2-cleanup-fails-after-successful-sync","text":"Symptoms : Job succeeds but cleanup fails Possible Causes : - PVC in use by other processes - Storage class deletion policies - Volume attachment issues Solutions : # Force PVC deletion (if safe) kubectl patch pvc <cloned-pvc-name> -p '{\"metadata\":{\"finalizers\":[]}}' --type=merge # Check for volume attachments kubectl get volumeattachment | grep <pv-name>","title":"2. Cleanup Fails After Successful Sync"},{"location":"automatic-cleanup/#3-metrics-not-recording-cleanup","text":"Symptoms : Cleanup happens but metrics not updated Diagnosis : # Check operator logs for metric errors kubectl logs -n datamover-operator-system deployment/datamover-operator-controller-manager | grep -i metric # Verify Prometheus scraping curl http://operator-metrics-service:8080/metrics | grep cleanup","title":"3. Metrics Not Recording Cleanup"},{"location":"automatic-cleanup/#debug-commands","text":"# Monitor cleanup process kubectl get datamover <name> -w # Check cleanup logs kubectl logs -n datamover-operator-system deployment/datamover-operator-controller-manager | grep -i \"cleanup\\|delete\" # List PVCs created by operator kubectl get pvc -l app.kubernetes.io/created-by=datamover-operator # Check PVC deletion events kubectl get events --field-selector involvedObject.kind=PersistentVolumeClaim","title":"Debug Commands"},{"location":"automatic-cleanup/#best-practices","text":"","title":"Best Practices"},{"location":"automatic-cleanup/#1-resource-planning","text":"Consider cleanup in resource planning: Temporary Storage : Plan for peak usage during clone creation Cleanup Timing : Consider cleanup duration in scheduling Quota Management : Account for temporary storage quota usage","title":"1. Resource Planning"},{"location":"automatic-cleanup/#2-monitoring","text":"Set up monitoring for cleanup operations: # Example Prometheus alert groups: - name: datamover.cleanup rules: - alert: DataMoverCleanupFailing expr: increase(datamover_cleanup_operations_total{status=\"failure\"}[5m]) > 0 for: 0m labels: severity: warning annotations: summary: \"DataMover cleanup operations are failing\" description: \"DataMover cleanup failures in namespace {{ $labels.namespace }}\"","title":"2. Monitoring"},{"location":"automatic-cleanup/#3-backup-verification","text":"Always verify backup success before cleanup: # Verify backup exists before cleanup completes rclone lsd s3:my-bucket/ | grep $(date +%Y-%m-%d)","title":"3. Backup Verification"},{"location":"automatic-cleanup/#4-testing","text":"Test cleanup behavior: # Test cleanup with small PVC apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: cleanup-test spec: sourcePvc: \"test-data\" secretName: \"test-credentials\" deletePvcAfterBackup: true","title":"4. Testing"},{"location":"automatic-cleanup/#5-documentation","text":"Document cleanup policies in your backup procedures: When cleanup is enabled/disabled Storage impact of cleanup decisions Recovery procedures if cleanup fails","title":"5. Documentation"},{"location":"automatic-cleanup/#advanced-scenarios","text":"","title":"Advanced Scenarios"},{"location":"automatic-cleanup/#conditional-cleanup","text":"Implement conditional cleanup based on backup verification: # Example: Only cleanup after backup verification apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: verified-cleanup spec: sourcePvc: \"critical-data\" secretName: \"storage-credentials\" deletePvcAfterBackup: true additionalEnv: - name: \"VERIFY_BACKUP\" value: \"true\"","title":"Conditional Cleanup"},{"location":"automatic-cleanup/#multi-destination-cleanup","text":"When backing up to multiple destinations: # Primary backup with cleanup apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: primary-backup spec: sourcePvc: \"important-data\" secretName: \"primary-storage\" deletePvcAfterBackup: false # Keep for secondary backup # Secondary backup without cleanup apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: secondary-backup spec: sourcePvc: \"important-data\" # Same source secretName: \"secondary-storage\" deletePvcAfterBackup: true # Cleanup after both complete","title":"Multi-Destination Cleanup"},{"location":"automatic-cleanup/#cleanup-with-lifecycle-management","text":"Integrate with external lifecycle management: #!/bin/bash # External cleanup verification script NAMESPACE=\"default\" DATAMOVER_NAME=\"my-backup\" # Wait for backup completion kubectl wait --for=condition=Complete datamover/$DATAMOVER_NAME -n $NAMESPACE --timeout=3600s # Verify backup in storage if rclone check s3:my-bucket/latest/ /verify/path/; then echo \"Backup verified, cleanup can proceed\" else echo \"Backup verification failed, manual intervention required\" exit 1 fi This comprehensive documentation covers all aspects of the automatic cleanup feature, providing users with the knowledge needed to effectively use and troubleshoot this functionality.","title":"Cleanup with Lifecycle Management"},{"location":"container-image-configuration/","text":"Container Image Configuration Both DataMover and DataMoverSchedule support custom container image configuration through the image field. This allows you to use custom rclone images or different versions. Image Specification The image field has three sub-fields: Field Type Default Description repository string ghcr.io/qjoly/datamover-rclone Full container image repository including registry tag string latest Image tag or version pullPolicy string Always Kubernetes image pull policy Pull Policies Valid values for pullPolicy : Always : Always pull the image, even if it exists locally IfNotPresent : Only pull if the image doesn't exist locally Never : Never pull the image, use local copy only Usage Examples Default Configuration When no image field is specified, the operator uses defaults: apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: default-image-example spec: sourcePvc: \"my-data\" secretName: \"storage-secret\" # No image field = uses defaults: # image: # repository: \"ghcr.io/qjoly/datamover-rclone\" # tag: \"latest\" # pullPolicy: \"Always\" Custom Image with Specific Tag apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: custom-image-example spec: sourcePvc: \"my-data\" secretName: \"storage-secret\" image: repository: \"ghcr.io/qjoly/datamover-rclone\" tag: \"v1.65.0\" pullPolicy: \"IfNotPresent\" Private Registry Image apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: private-registry-example spec: sourcePvc: \"my-data\" secretName: \"storage-secret\" image: repository: \"my-company.com/tools/rclone\" tag: \"enterprise-v2.1.0\" pullPolicy: \"Always\" DataMoverSchedule Examples Production Schedule with Stable Image apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMoverSchedule metadata: name: production-backup spec: schedule: \"0 2 * * *\" sourcePvc: \"production-data\" secretName: \"backup-credentials\" image: repository: \"ghcr.io/qjoly/datamover-rclone\" tag: \"v1.65.0\" # Pinned version for stability pullPolicy: \"IfNotPresent\" # Avoid unnecessary pulls successfulJobsHistoryLimit: 7 Testing Schedule with Latest Features apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMoverSchedule metadata: name: testing-backup spec: schedule: \"0 */6 * * *\" # Every 6 hours sourcePvc: \"test-data\" secretName: \"test-credentials\" image: repository: \"ghcr.io/qjoly/datamover-rclone\" tag: \"latest\" pullPolicy: \"Always\" # Always get latest features successfulJobsHistoryLimit: 3 Custom Images for Alternative Software While DataMover was designed with rclone in mind, it's possible to use custom container images that implement other backup or data synchronization tools. This flexibility allows you to integrate your preferred data management software while leveraging DataMover's PVC cloning and job orchestration capabilities. Requirements for Custom Images Any custom image used with DataMover must comply with the following requirements: Data Location : The image must expect source data to be mounted at /data/ Configuration via Environment Variables : All configuration must be provided through environment variables (no config files) Exit Codes : Must use standard exit codes (0 for success, non-zero for failure) Signal Handling : Should handle SIGTERM gracefully for clean shutdown Environment Variables Your custom image will receive all environment variables from the secret specified in secretName , plus any additional variables defined in additionalEnv . Common patterns include: Storage credentials (AWS keys, API tokens, etc.) Destination configuration (bucket names, endpoints, etc.) Behavioral flags and options Feature Compatibility Limitations \u26a0\ufe0f Important : Some DataMover features may not work with custom images, depending on the software implementation: addTimestampPrefix : Only works if your image supports creating timestamped directory structures Error handling : Different tools may handle errors differently Example: Custom Image with rsync FROM alpine:3.18 # Install rsync and required tools RUN apk add --no-cache rsync openssh-client # Custom entrypoint script (that handles rsync) COPY entrypoint.sh /usr/local/bin/ RUN chmod +x /usr/local/bin/entrypoint.sh USER backup WORKDIR /data ENTRYPOINT [\"/usr/local/bin/entrypoint.sh\"] Example: Custom Image Configuration apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: custom-backup-tool spec: sourcePvc: \"my-data\" secretName: \"backup-credentials\" image: repository: \"my-registry.com/custom-backup\" tag: \"v2.1.0\" pullPolicy: \"IfNotPresent\" # Note: addTimestampPrefix may not work with custom tools addTimestampPrefix: false additionalEnv: - name: \"BACKUP_MODE\" value: \"incremental\" - name: \"COMPRESSION\" value: \"gzip\"","title":"Container Image Configuration"},{"location":"container-image-configuration/#container-image-configuration","text":"Both DataMover and DataMoverSchedule support custom container image configuration through the image field. This allows you to use custom rclone images or different versions.","title":"Container Image Configuration"},{"location":"container-image-configuration/#image-specification","text":"The image field has three sub-fields: Field Type Default Description repository string ghcr.io/qjoly/datamover-rclone Full container image repository including registry tag string latest Image tag or version pullPolicy string Always Kubernetes image pull policy","title":"Image Specification"},{"location":"container-image-configuration/#pull-policies","text":"Valid values for pullPolicy : Always : Always pull the image, even if it exists locally IfNotPresent : Only pull if the image doesn't exist locally Never : Never pull the image, use local copy only","title":"Pull Policies"},{"location":"container-image-configuration/#usage-examples","text":"","title":"Usage Examples"},{"location":"container-image-configuration/#default-configuration","text":"When no image field is specified, the operator uses defaults: apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: default-image-example spec: sourcePvc: \"my-data\" secretName: \"storage-secret\" # No image field = uses defaults: # image: # repository: \"ghcr.io/qjoly/datamover-rclone\" # tag: \"latest\" # pullPolicy: \"Always\"","title":"Default Configuration"},{"location":"container-image-configuration/#custom-image-with-specific-tag","text":"apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: custom-image-example spec: sourcePvc: \"my-data\" secretName: \"storage-secret\" image: repository: \"ghcr.io/qjoly/datamover-rclone\" tag: \"v1.65.0\" pullPolicy: \"IfNotPresent\"","title":"Custom Image with Specific Tag"},{"location":"container-image-configuration/#private-registry-image","text":"apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: private-registry-example spec: sourcePvc: \"my-data\" secretName: \"storage-secret\" image: repository: \"my-company.com/tools/rclone\" tag: \"enterprise-v2.1.0\" pullPolicy: \"Always\"","title":"Private Registry Image"},{"location":"container-image-configuration/#datamoverschedule-examples","text":"","title":"DataMoverSchedule Examples"},{"location":"container-image-configuration/#production-schedule-with-stable-image","text":"apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMoverSchedule metadata: name: production-backup spec: schedule: \"0 2 * * *\" sourcePvc: \"production-data\" secretName: \"backup-credentials\" image: repository: \"ghcr.io/qjoly/datamover-rclone\" tag: \"v1.65.0\" # Pinned version for stability pullPolicy: \"IfNotPresent\" # Avoid unnecessary pulls successfulJobsHistoryLimit: 7","title":"Production Schedule with Stable Image"},{"location":"container-image-configuration/#testing-schedule-with-latest-features","text":"apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMoverSchedule metadata: name: testing-backup spec: schedule: \"0 */6 * * *\" # Every 6 hours sourcePvc: \"test-data\" secretName: \"test-credentials\" image: repository: \"ghcr.io/qjoly/datamover-rclone\" tag: \"latest\" pullPolicy: \"Always\" # Always get latest features successfulJobsHistoryLimit: 3","title":"Testing Schedule with Latest Features"},{"location":"container-image-configuration/#custom-images-for-alternative-software","text":"While DataMover was designed with rclone in mind, it's possible to use custom container images that implement other backup or data synchronization tools. This flexibility allows you to integrate your preferred data management software while leveraging DataMover's PVC cloning and job orchestration capabilities.","title":"Custom Images for Alternative Software"},{"location":"container-image-configuration/#requirements-for-custom-images","text":"Any custom image used with DataMover must comply with the following requirements: Data Location : The image must expect source data to be mounted at /data/ Configuration via Environment Variables : All configuration must be provided through environment variables (no config files) Exit Codes : Must use standard exit codes (0 for success, non-zero for failure) Signal Handling : Should handle SIGTERM gracefully for clean shutdown","title":"Requirements for Custom Images"},{"location":"container-image-configuration/#environment-variables","text":"Your custom image will receive all environment variables from the secret specified in secretName , plus any additional variables defined in additionalEnv . Common patterns include: Storage credentials (AWS keys, API tokens, etc.) Destination configuration (bucket names, endpoints, etc.) Behavioral flags and options","title":"Environment Variables"},{"location":"container-image-configuration/#feature-compatibility-limitations","text":"\u26a0\ufe0f Important : Some DataMover features may not work with custom images, depending on the software implementation: addTimestampPrefix : Only works if your image supports creating timestamped directory structures Error handling : Different tools may handle errors differently","title":"Feature Compatibility Limitations"},{"location":"container-image-configuration/#example-custom-image-with-rsync","text":"FROM alpine:3.18 # Install rsync and required tools RUN apk add --no-cache rsync openssh-client # Custom entrypoint script (that handles rsync) COPY entrypoint.sh /usr/local/bin/ RUN chmod +x /usr/local/bin/entrypoint.sh USER backup WORKDIR /data ENTRYPOINT [\"/usr/local/bin/entrypoint.sh\"]","title":"Example: Custom Image with rsync"},{"location":"container-image-configuration/#example-custom-image-configuration","text":"apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: custom-backup-tool spec: sourcePvc: \"my-data\" secretName: \"backup-credentials\" image: repository: \"my-registry.com/custom-backup\" tag: \"v2.1.0\" pullPolicy: \"IfNotPresent\" # Note: addTimestampPrefix may not work with custom tools addTimestampPrefix: false additionalEnv: - name: \"BACKUP_MODE\" value: \"incremental\" - name: \"COMPRESSION\" value: \"gzip\"","title":"Example: Custom Image Configuration"},{"location":"data-synchronization/","text":"Data Synchronization This document covers the data synchronization capabilities of the DataMover Operator, including rclone integration and supported storage backends. Overview The DataMover Operator uses rclone to synchronize data from cloned PVCs to remote storage backends. This provides a robust, well-tested solution for data transfer with support for numerous cloud and on-premises storage systems. Rclone Integration Container Image The operator uses a custom rclone container image: ttl.sh/rclone_op:latest This image includes: - Latest rclone binary - Custom entrypoint script for operator integration - Support for timestamp prefix functionality - Optimized for Kubernetes environments Synchronization Process PVC Mounting : Cloned PVC is mounted at /data/ in the rclone container Configuration : Storage credentials loaded from Kubernetes secrets Sync Execution : Rclone syncs /data/ to configured remote destination Completion : Job completes with success/failure status Supported Storage Backends MinIO Configuration : AWS_ACCESS_KEY_ID: minio-access-key AWS_SECRET_ACCESS_KEY: minio-secret-key AWS_REGION: us-east-1 BUCKET_HOST: minio.example.com BUCKET_NAME: backups BUCKET_PORT: \"9000\" TLS_HOST: \"false\" Configuration Management Secret Structure Storage credentials are provided via Kubernetes secrets: apiVersion: v1 kind: Secret metadata: name: storage-credentials type: Opaque data: # Base64-encoded values AWS_ACCESS_KEY_ID: <encoded-access-key> AWS_SECRET_ACCESS_KEY: <encoded-secret-key> AWS_REGION: <encoded-region> BUCKET_HOST: <encoded-host> BUCKET_NAME: <encoded-bucket> BUCKET_PORT: <encoded-port> TLS_HOST: <encoded-true-or-false> Environment Variables The rclone container receives configuration through environment variables: # Storage backend configuration AWS_ACCESS_KEY_ID=your-access-key AWS_SECRET_ACCESS_KEY=your-secret-key AWS_REGION=us-west-2 BUCKET_HOST=s3.amazonaws.com BUCKET_NAME=my-backup-bucket BUCKET_PORT=443 TLS_HOST=true # Operator-specific configuration ADD_TIMESTAMP_PREFIX=true # Enable timestamp organization Additional Environment Variables You can provide additional environment variables through the DataMover spec: apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: custom-sync spec: sourcePvc: \"app-data\" secretName: \"storage-credentials\" additionalEnv: - name: \"RCLONE_TRANSFERS\" value: \"8\" - name: \"RCLONE_CHECKERS\" value: \"16\" - name: \"CUSTOM_PATH_PREFIX\" value: \"production\" Synchronization Features Timestamp Organization When addTimestampPrefix: true is set, data is organized with timestamps: bucket/ \u251c\u2500\u2500 2024-08-06-143052/ # Timestamp folder \u2502 \u251c\u2500\u2500 app/ \u2502 \u251c\u2500\u2500 data/ \u2502 \u2514\u2500\u2500 logs/ \u2514\u2500\u2500 2024-08-06-151225/ # Another backup \u251c\u2500\u2500 app/ \u251c\u2500\u2500 data/ \u2514\u2500\u2500 logs/ Format : YYYY-MM-DD-HHMMSS Benefits : - Point-in-time recovery - Historical backup tracking - Organized storage structure - Easy cleanup of old backups Incremental Synchronization Rclone performs incremental synchronization by default: Changed files : Only modified files are transferred New files : New files are uploaded Deleted files : Files deleted from source are removed from destination Checksums : File integrity verified through checksums Metrics Collection The operator tracks synchronization metrics: # Sync operation counters datamover_data_sync_operations_total{status=\"success\"} datamover_data_sync_operations_total{status=\"failure\"} # Phase duration tracking datamover_phase_duration_seconds{phase=\"CreatingPod\"} Performance Tuning Transfer Optimization Optimize transfer performance based on your environment: High Bandwidth Networks additionalEnv: - name: \"RCLONE_TRANSFERS\" value: \"8\" # More parallel transfers - name: \"RCLONE_CHECKERS\" value: \"16\" # More parallel checks Limited Bandwidth Networks additionalEnv: - name: \"RCLONE_TRANSFERS\" value: \"2\" # Fewer parallel transfers - name: \"RCLONE_BW_LIMIT\" value: \"10M\" # Bandwidth limit Large Files additionalEnv: - name: \"RCLONE_MULTI_THREAD_CUTOFF\" value: \"50M\" # Multi-thread for files > 50MB - name: \"RCLONE_MULTI_THREAD_STREAMS\" value: \"4\" # 4 streams per large file Resource Allocation Configure appropriate resources for sync jobs: # In job template (operator configuration) resources: requests: memory: \"512Mi\" # Base memory for rclone cpu: \"200m\" # Base CPU for operations limits: memory: \"2Gi\" # Maximum memory (adjust for large files) cpu: \"1000m\" # Maximum CPU for parallel operations Error Handling Common Sync Errors 1. Authentication Failures Error : Failed to configure s3 backend: NoCredentialsErr Solutions : - Verify secret credentials are correct - Check credential encoding (base64) - Validate IAM permissions for storage access 2. Network Connectivity Issues Error : Failed to copy: connection timeout Solutions : - Check network policies and firewall rules - Verify storage endpoint accessibility - Consider bandwidth limitations 3. Storage Permission Issues Error : AccessDenied: Access Denied Solutions : - Verify bucket/container permissions - Check IAM roles and policies - Validate storage account access keys 4. Storage Space Issues Error : No space left on device Solutions : - Check storage quota limits - Verify available space in destination - Consider data compression options Retry Strategy Rclone has built-in retry mechanisms: File-level retries : Individual file transfer failures Operation retries : Overall operation failures Exponential backoff : Increasing delays between retries Combined with Kubernetes Job retries, this provides robust error recovery. Security Considerations Credential Management Store credentials only in Kubernetes secrets Use least-privilege access policies Rotate credentials regularly Monitor credential usage Data Encryption In Transit Enable TLS for all connections ( TLS_HOST: \"true\" ) Use encrypted storage endpoints Verify certificate validation At Rest Configure server-side encryption Use customer-managed encryption keys when available Enable storage backend encryption features Network Security Use private endpoints when possible Implement network policies for job pods Restrict egress traffic to required destinations Monitor network access patterns Troubleshooting Synchronization Diagnosis Commands # Check rclone job status kubectl get jobs -l app.kubernetes.io/created-by=datamover-operator # View sync logs kubectl logs job/verify-<pvc-name> # Check secret configuration kubectl get secret <secret-name> -o yaml # Test storage connectivity kubectl run rclone-test --rm -it --image=rclone/rclone -- rclone lsd remote: Debug Configuration For debugging sync issues, add debug environment variables: additionalEnv: - name: \"RCLONE_VERBOSE\" value: \"2\" # Increase verbosity - name: \"RCLONE_LOG_LEVEL\" value: \"DEBUG\" # Debug logging - name: \"RCLONE_DUMP\" value: \"headers\" # Dump HTTP headers Performance Analysis Monitor sync performance: # Check transfer statistics kubectl logs job/verify-<pvc-name> | grep \"Transferred:\" # Monitor resource usage kubectl top pod <rclone-pod-name> # Check storage backend performance # (depends on storage backend monitoring tools) Best Practices 1. Configuration Management Use separate secrets for different environments Validate configuration before creating DataMover resources Document storage backend requirements 2. Performance Optimization Tune rclone settings for your environment Monitor transfer performance and adjust accordingly Consider storage backend limitations 3. Security Follow principle of least privilege Enable encryption in transit and at rest Regular security audits of configurations 4. Monitoring Set up alerts for sync failures Monitor sync duration trends Track storage usage patterns 5. Testing Test sync operations with sample data Validate backup integrity Test restore procedures regularly","title":"Data Synchronization"},{"location":"data-synchronization/#data-synchronization","text":"This document covers the data synchronization capabilities of the DataMover Operator, including rclone integration and supported storage backends.","title":"Data Synchronization"},{"location":"data-synchronization/#overview","text":"The DataMover Operator uses rclone to synchronize data from cloned PVCs to remote storage backends. This provides a robust, well-tested solution for data transfer with support for numerous cloud and on-premises storage systems.","title":"Overview"},{"location":"data-synchronization/#rclone-integration","text":"","title":"Rclone Integration"},{"location":"data-synchronization/#container-image","text":"The operator uses a custom rclone container image: ttl.sh/rclone_op:latest This image includes: - Latest rclone binary - Custom entrypoint script for operator integration - Support for timestamp prefix functionality - Optimized for Kubernetes environments","title":"Container Image"},{"location":"data-synchronization/#synchronization-process","text":"PVC Mounting : Cloned PVC is mounted at /data/ in the rclone container Configuration : Storage credentials loaded from Kubernetes secrets Sync Execution : Rclone syncs /data/ to configured remote destination Completion : Job completes with success/failure status","title":"Synchronization Process"},{"location":"data-synchronization/#supported-storage-backends","text":"","title":"Supported Storage Backends"},{"location":"data-synchronization/#minio","text":"Configuration : AWS_ACCESS_KEY_ID: minio-access-key AWS_SECRET_ACCESS_KEY: minio-secret-key AWS_REGION: us-east-1 BUCKET_HOST: minio.example.com BUCKET_NAME: backups BUCKET_PORT: \"9000\" TLS_HOST: \"false\"","title":"MinIO"},{"location":"data-synchronization/#configuration-management","text":"","title":"Configuration Management"},{"location":"data-synchronization/#secret-structure","text":"Storage credentials are provided via Kubernetes secrets: apiVersion: v1 kind: Secret metadata: name: storage-credentials type: Opaque data: # Base64-encoded values AWS_ACCESS_KEY_ID: <encoded-access-key> AWS_SECRET_ACCESS_KEY: <encoded-secret-key> AWS_REGION: <encoded-region> BUCKET_HOST: <encoded-host> BUCKET_NAME: <encoded-bucket> BUCKET_PORT: <encoded-port> TLS_HOST: <encoded-true-or-false>","title":"Secret Structure"},{"location":"data-synchronization/#environment-variables","text":"The rclone container receives configuration through environment variables: # Storage backend configuration AWS_ACCESS_KEY_ID=your-access-key AWS_SECRET_ACCESS_KEY=your-secret-key AWS_REGION=us-west-2 BUCKET_HOST=s3.amazonaws.com BUCKET_NAME=my-backup-bucket BUCKET_PORT=443 TLS_HOST=true # Operator-specific configuration ADD_TIMESTAMP_PREFIX=true # Enable timestamp organization","title":"Environment Variables"},{"location":"data-synchronization/#additional-environment-variables","text":"You can provide additional environment variables through the DataMover spec: apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: custom-sync spec: sourcePvc: \"app-data\" secretName: \"storage-credentials\" additionalEnv: - name: \"RCLONE_TRANSFERS\" value: \"8\" - name: \"RCLONE_CHECKERS\" value: \"16\" - name: \"CUSTOM_PATH_PREFIX\" value: \"production\"","title":"Additional Environment Variables"},{"location":"data-synchronization/#synchronization-features","text":"","title":"Synchronization Features"},{"location":"data-synchronization/#timestamp-organization","text":"When addTimestampPrefix: true is set, data is organized with timestamps: bucket/ \u251c\u2500\u2500 2024-08-06-143052/ # Timestamp folder \u2502 \u251c\u2500\u2500 app/ \u2502 \u251c\u2500\u2500 data/ \u2502 \u2514\u2500\u2500 logs/ \u2514\u2500\u2500 2024-08-06-151225/ # Another backup \u251c\u2500\u2500 app/ \u251c\u2500\u2500 data/ \u2514\u2500\u2500 logs/ Format : YYYY-MM-DD-HHMMSS Benefits : - Point-in-time recovery - Historical backup tracking - Organized storage structure - Easy cleanup of old backups","title":"Timestamp Organization"},{"location":"data-synchronization/#incremental-synchronization","text":"Rclone performs incremental synchronization by default: Changed files : Only modified files are transferred New files : New files are uploaded Deleted files : Files deleted from source are removed from destination Checksums : File integrity verified through checksums","title":"Incremental Synchronization"},{"location":"data-synchronization/#metrics-collection","text":"The operator tracks synchronization metrics: # Sync operation counters datamover_data_sync_operations_total{status=\"success\"} datamover_data_sync_operations_total{status=\"failure\"} # Phase duration tracking datamover_phase_duration_seconds{phase=\"CreatingPod\"}","title":"Metrics Collection"},{"location":"data-synchronization/#performance-tuning","text":"","title":"Performance Tuning"},{"location":"data-synchronization/#transfer-optimization","text":"Optimize transfer performance based on your environment:","title":"Transfer Optimization"},{"location":"data-synchronization/#high-bandwidth-networks","text":"additionalEnv: - name: \"RCLONE_TRANSFERS\" value: \"8\" # More parallel transfers - name: \"RCLONE_CHECKERS\" value: \"16\" # More parallel checks","title":"High Bandwidth Networks"},{"location":"data-synchronization/#limited-bandwidth-networks","text":"additionalEnv: - name: \"RCLONE_TRANSFERS\" value: \"2\" # Fewer parallel transfers - name: \"RCLONE_BW_LIMIT\" value: \"10M\" # Bandwidth limit","title":"Limited Bandwidth Networks"},{"location":"data-synchronization/#large-files","text":"additionalEnv: - name: \"RCLONE_MULTI_THREAD_CUTOFF\" value: \"50M\" # Multi-thread for files > 50MB - name: \"RCLONE_MULTI_THREAD_STREAMS\" value: \"4\" # 4 streams per large file","title":"Large Files"},{"location":"data-synchronization/#resource-allocation","text":"Configure appropriate resources for sync jobs: # In job template (operator configuration) resources: requests: memory: \"512Mi\" # Base memory for rclone cpu: \"200m\" # Base CPU for operations limits: memory: \"2Gi\" # Maximum memory (adjust for large files) cpu: \"1000m\" # Maximum CPU for parallel operations","title":"Resource Allocation"},{"location":"data-synchronization/#error-handling","text":"","title":"Error Handling"},{"location":"data-synchronization/#common-sync-errors","text":"","title":"Common Sync Errors"},{"location":"data-synchronization/#1-authentication-failures","text":"Error : Failed to configure s3 backend: NoCredentialsErr Solutions : - Verify secret credentials are correct - Check credential encoding (base64) - Validate IAM permissions for storage access","title":"1. Authentication Failures"},{"location":"data-synchronization/#2-network-connectivity-issues","text":"Error : Failed to copy: connection timeout Solutions : - Check network policies and firewall rules - Verify storage endpoint accessibility - Consider bandwidth limitations","title":"2. Network Connectivity Issues"},{"location":"data-synchronization/#3-storage-permission-issues","text":"Error : AccessDenied: Access Denied Solutions : - Verify bucket/container permissions - Check IAM roles and policies - Validate storage account access keys","title":"3. Storage Permission Issues"},{"location":"data-synchronization/#4-storage-space-issues","text":"Error : No space left on device Solutions : - Check storage quota limits - Verify available space in destination - Consider data compression options","title":"4. Storage Space Issues"},{"location":"data-synchronization/#retry-strategy","text":"Rclone has built-in retry mechanisms: File-level retries : Individual file transfer failures Operation retries : Overall operation failures Exponential backoff : Increasing delays between retries Combined with Kubernetes Job retries, this provides robust error recovery.","title":"Retry Strategy"},{"location":"data-synchronization/#security-considerations","text":"","title":"Security Considerations"},{"location":"data-synchronization/#credential-management","text":"Store credentials only in Kubernetes secrets Use least-privilege access policies Rotate credentials regularly Monitor credential usage","title":"Credential Management"},{"location":"data-synchronization/#data-encryption","text":"","title":"Data Encryption"},{"location":"data-synchronization/#in-transit","text":"Enable TLS for all connections ( TLS_HOST: \"true\" ) Use encrypted storage endpoints Verify certificate validation","title":"In Transit"},{"location":"data-synchronization/#at-rest","text":"Configure server-side encryption Use customer-managed encryption keys when available Enable storage backend encryption features","title":"At Rest"},{"location":"data-synchronization/#network-security","text":"Use private endpoints when possible Implement network policies for job pods Restrict egress traffic to required destinations Monitor network access patterns","title":"Network Security"},{"location":"data-synchronization/#troubleshooting-synchronization","text":"","title":"Troubleshooting Synchronization"},{"location":"data-synchronization/#diagnosis-commands","text":"# Check rclone job status kubectl get jobs -l app.kubernetes.io/created-by=datamover-operator # View sync logs kubectl logs job/verify-<pvc-name> # Check secret configuration kubectl get secret <secret-name> -o yaml # Test storage connectivity kubectl run rclone-test --rm -it --image=rclone/rclone -- rclone lsd remote:","title":"Diagnosis Commands"},{"location":"data-synchronization/#debug-configuration","text":"For debugging sync issues, add debug environment variables: additionalEnv: - name: \"RCLONE_VERBOSE\" value: \"2\" # Increase verbosity - name: \"RCLONE_LOG_LEVEL\" value: \"DEBUG\" # Debug logging - name: \"RCLONE_DUMP\" value: \"headers\" # Dump HTTP headers","title":"Debug Configuration"},{"location":"data-synchronization/#performance-analysis","text":"Monitor sync performance: # Check transfer statistics kubectl logs job/verify-<pvc-name> | grep \"Transferred:\" # Monitor resource usage kubectl top pod <rclone-pod-name> # Check storage backend performance # (depends on storage backend monitoring tools)","title":"Performance Analysis"},{"location":"data-synchronization/#best-practices","text":"","title":"Best Practices"},{"location":"data-synchronization/#1-configuration-management","text":"Use separate secrets for different environments Validate configuration before creating DataMover resources Document storage backend requirements","title":"1. Configuration Management"},{"location":"data-synchronization/#2-performance-optimization","text":"Tune rclone settings for your environment Monitor transfer performance and adjust accordingly Consider storage backend limitations","title":"2. Performance Optimization"},{"location":"data-synchronization/#3-security","text":"Follow principle of least privilege Enable encryption in transit and at rest Regular security audits of configurations","title":"3. Security"},{"location":"data-synchronization/#4-monitoring","text":"Set up alerts for sync failures Monitor sync duration trends Track storage usage patterns","title":"4. Monitoring"},{"location":"data-synchronization/#5-testing","text":"Test sync operations with sample data Validate backup integrity Test restore procedures regularly","title":"5. Testing"},{"location":"metrics-and-monitoring/","text":"Metrics and Monitoring This document covers the comprehensive Prometheus metrics and monitoring capabilities provided by the DataMover Operator. Overview The DataMover Operator provides detailed Prometheus metrics for monitoring backup operations, performance tracking, and operational observability. These metrics enable effective monitoring, alerting, and troubleshooting of data movement operations. Available Metrics Operation Metrics datamover_operations_total Counter tracking total operations by phase and status. Labels : - phase : Operation phase (CreatingClonedPVC, CreatingPod, CleaningUp) - status : Operation status (started, success, failure) - namespace : Kubernetes namespace Examples : datamover_operations_total{phase=\"CreatingClonedPVC\", status=\"started\", namespace=\"default\"} 10 datamover_operations_total{phase=\"CreatingClonedPVC\", status=\"success\", namespace=\"default\"} 9 datamover_operations_total{phase=\"CreatingClonedPVC\", status=\"failure\", namespace=\"default\"} 1 datamover_phase_duration_seconds Histogram tracking phase execution duration. Labels : - phase : Operation phase - namespace : Kubernetes namespace Buckets : 0.1, 0.5, 1, 2.5, 5, 10, 30, 60, 120, 300, 600, 1200, +Inf seconds Examples : datamover_phase_duration_seconds_bucket{phase=\"CreatingPod\", namespace=\"default\", le=\"300\"} 45 datamover_phase_duration_seconds_sum{phase=\"CreatingPod\", namespace=\"default\"} 12847.3 datamover_phase_duration_seconds_count{phase=\"CreatingPod\", namespace=\"default\"} 50 Cleanup Metrics datamover_cleanup_operations_total Counter tracking PVC cleanup operations. Labels : - status : Cleanup status (success, failure) - namespace : Kubernetes namespace Examples : datamover_cleanup_operations_total{status=\"success\", namespace=\"default\"} 25 datamover_cleanup_operations_total{status=\"failure\", namespace=\"default\"} 2 Job Metrics datamover_pod_creation_operations_total Counter tracking job/pod creation operations. Labels : - status : Operation status (started, success, failure) - namespace : Kubernetes namespace Examples : datamover_pod_creation_operations_total{status=\"started\", namespace=\"default\"} 30 datamover_pod_creation_operations_total{status=\"success\", namespace=\"default\"} 28 datamover_pod_creation_operations_total{status=\"failure\", namespace=\"default\"} 2 datamover_data_sync_operations_total Counter tracking data synchronization operations. Labels : - status : Sync status (success, failure) - namespace : Kubernetes namespace Examples : datamover_data_sync_operations_total{status=\"success\", namespace=\"default\"} 25 datamover_data_sync_operations_total{status=\"failure\", namespace=\"default\"} 3 Metric Collection Prometheus Configuration Configure Prometheus to scrape DataMover metrics: # prometheus.yml scrape_configs: - job_name: 'datamover-operator' static_configs: - targets: ['datamover-operator-controller-manager-metrics-service:8080'] scrape_interval: 30s metrics_path: /metrics ServiceMonitor (Prometheus Operator) For Prometheus Operator deployments: apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: datamover-operator namespace: datamover-operator-system spec: selector: matchLabels: control-plane: controller-manager endpoints: - port: https scheme: https path: /metrics tlsConfig: insecureSkipVerify: true Manual Metric Access Access metrics directly: # Port-forward to metrics endpoint kubectl port-forward -n datamover-operator-system service/datamover-operator-controller-manager-metrics-service 8080:8080 # Query metrics curl http://localhost:8080/metrics | grep datamover Monitoring Dashboards Grafana Dashboard Example Create comprehensive Grafana dashboard: { \"dashboard\": { \"title\": \"DataMover Operator\", \"panels\": [ { \"title\": \"Operation Success Rate\", \"type\": \"stat\", \"targets\": [ { \"expr\": \"rate(datamover_operations_total{status=\\\"success\\\"}[5m]) / rate(datamover_operations_total[5m]) * 100\" } ] }, { \"title\": \"Phase Duration\", \"type\": \"graph\", \"targets\": [ { \"expr\": \"histogram_quantile(0.95, rate(datamover_phase_duration_seconds_bucket[5m]))\" } ] }, { \"title\": \"Operations by Phase\", \"type\": \"graph\", \"targets\": [ { \"expr\": \"rate(datamover_operations_total[5m])\" } ] } ] } } Key Performance Indicators (KPIs) Monitor these critical metrics: Success Rate : Overall operation success percentage Phase Duration : Time taken for each operation phase Failure Rate : Operations failing per time period Cleanup Success : PVC cleanup operation success rate Dashboard Panels Success Rate Panel # Overall success rate rate(datamover_operations_total{status=\"success\"}[5m]) / rate(datamover_operations_total[5m]) * 100 Average Phase Duration Panel # Average phase duration by phase rate(datamover_phase_duration_seconds_sum[5m]) / rate(datamover_phase_duration_seconds_count[5m]) Operations Volume Panel # Operations per minute by phase rate(datamover_operations_total[1m]) * 60 Cleanup Success Rate Panel # Cleanup success rate rate(datamover_cleanup_operations_total{status=\"success\"}[5m]) / rate(datamover_cleanup_operations_total[5m]) * 100 Alerting Prometheus Alerting Rules Configure alerts for operational issues: groups: - name: datamover.rules rules: # High failure rate alert - alert: DataMoverHighFailureRate expr: | ( rate(datamover_operations_total{status=\"failure\"}[5m]) / rate(datamover_operations_total[5m]) ) > 0.1 for: 2m labels: severity: warning annotations: summary: \"DataMover high failure rate\" description: \"DataMover failure rate is above 10% in namespace {{ $labels.namespace }}\" # Long-running operations alert - alert: DataMoverLongRunningOperation expr: | histogram_quantile(0.95, rate(datamover_phase_duration_seconds_bucket[5m])) > 1800 for: 5m labels: severity: warning annotations: summary: \"DataMover operations taking too long\" description: \"95th percentile of DataMover operations exceeds 30 minutes\" # Cleanup failures alert - alert: DataMoverCleanupFailures expr: | rate(datamover_cleanup_operations_total{status=\"failure\"}[5m]) > 0 for: 1m labels: severity: warning annotations: summary: \"DataMover cleanup operations failing\" description: \"PVC cleanup operations are failing in namespace {{ $labels.namespace }}\" # No operations alert (for scheduled backups) - alert: DataMoverNoOperations expr: | rate(datamover_operations_total[1h]) == 0 for: 2h labels: severity: info annotations: summary: \"No DataMover operations detected\" description: \"No DataMover operations in the last hour\" # Job retry exhaustion alert - alert: DataMoverJobRetriesExhausted expr: | rate(datamover_pod_creation_operations_total{status=\"failure\"}[5m]) > 0 for: 0m labels: severity: critical annotations: summary: \"DataMover jobs exhausting retries\" description: \"DataMover jobs are failing after all retry attempts\" Alert Severity Levels Critical : Immediate attention required - Job retries exhausted - Complete operation failures - Security violations Warning : Investigation needed - High failure rates - Performance degradation - Cleanup failures Info : Awareness notifications - No operations detected - Configuration changes - Routine maintenance Performance Monitoring Baseline Metrics Establish baseline performance metrics: Metric Typical Range Alert Threshold PVC Clone Creation 30s - 5min > 10min Job Execution 1min - 30min > 60min Cleanup Operation 5s - 30s > 2min Overall Success Rate > 95% < 90% Performance Queries Average Operation Duration # Average duration by phase over last hour avg_over_time( rate(datamover_phase_duration_seconds_sum[5m])[1h:5m] ) / avg_over_time( rate(datamover_phase_duration_seconds_count[5m])[1h:5m] ) Success Rate Trend # Success rate trend over 24 hours rate(datamover_operations_total{status=\"success\"}[1h]) / rate(datamover_operations_total[1h]) Resource Utilization Correlation # Correlate phase duration with resource usage rate(datamover_phase_duration_seconds_sum[5m]) and on() rate(container_cpu_usage_seconds_total{pod=~\"datamover-operator.*\"}[5m])","title":"Metrics and Monitoring"},{"location":"metrics-and-monitoring/#metrics-and-monitoring","text":"This document covers the comprehensive Prometheus metrics and monitoring capabilities provided by the DataMover Operator.","title":"Metrics and Monitoring"},{"location":"metrics-and-monitoring/#overview","text":"The DataMover Operator provides detailed Prometheus metrics for monitoring backup operations, performance tracking, and operational observability. These metrics enable effective monitoring, alerting, and troubleshooting of data movement operations.","title":"Overview"},{"location":"metrics-and-monitoring/#available-metrics","text":"","title":"Available Metrics"},{"location":"metrics-and-monitoring/#operation-metrics","text":"","title":"Operation Metrics"},{"location":"metrics-and-monitoring/#datamover_operations_total","text":"Counter tracking total operations by phase and status. Labels : - phase : Operation phase (CreatingClonedPVC, CreatingPod, CleaningUp) - status : Operation status (started, success, failure) - namespace : Kubernetes namespace Examples : datamover_operations_total{phase=\"CreatingClonedPVC\", status=\"started\", namespace=\"default\"} 10 datamover_operations_total{phase=\"CreatingClonedPVC\", status=\"success\", namespace=\"default\"} 9 datamover_operations_total{phase=\"CreatingClonedPVC\", status=\"failure\", namespace=\"default\"} 1","title":"datamover_operations_total"},{"location":"metrics-and-monitoring/#datamover_phase_duration_seconds","text":"Histogram tracking phase execution duration. Labels : - phase : Operation phase - namespace : Kubernetes namespace Buckets : 0.1, 0.5, 1, 2.5, 5, 10, 30, 60, 120, 300, 600, 1200, +Inf seconds Examples : datamover_phase_duration_seconds_bucket{phase=\"CreatingPod\", namespace=\"default\", le=\"300\"} 45 datamover_phase_duration_seconds_sum{phase=\"CreatingPod\", namespace=\"default\"} 12847.3 datamover_phase_duration_seconds_count{phase=\"CreatingPod\", namespace=\"default\"} 50","title":"datamover_phase_duration_seconds"},{"location":"metrics-and-monitoring/#cleanup-metrics","text":"","title":"Cleanup Metrics"},{"location":"metrics-and-monitoring/#datamover_cleanup_operations_total","text":"Counter tracking PVC cleanup operations. Labels : - status : Cleanup status (success, failure) - namespace : Kubernetes namespace Examples : datamover_cleanup_operations_total{status=\"success\", namespace=\"default\"} 25 datamover_cleanup_operations_total{status=\"failure\", namespace=\"default\"} 2","title":"datamover_cleanup_operations_total"},{"location":"metrics-and-monitoring/#job-metrics","text":"","title":"Job Metrics"},{"location":"metrics-and-monitoring/#datamover_pod_creation_operations_total","text":"Counter tracking job/pod creation operations. Labels : - status : Operation status (started, success, failure) - namespace : Kubernetes namespace Examples : datamover_pod_creation_operations_total{status=\"started\", namespace=\"default\"} 30 datamover_pod_creation_operations_total{status=\"success\", namespace=\"default\"} 28 datamover_pod_creation_operations_total{status=\"failure\", namespace=\"default\"} 2","title":"datamover_pod_creation_operations_total"},{"location":"metrics-and-monitoring/#datamover_data_sync_operations_total","text":"Counter tracking data synchronization operations. Labels : - status : Sync status (success, failure) - namespace : Kubernetes namespace Examples : datamover_data_sync_operations_total{status=\"success\", namespace=\"default\"} 25 datamover_data_sync_operations_total{status=\"failure\", namespace=\"default\"} 3","title":"datamover_data_sync_operations_total"},{"location":"metrics-and-monitoring/#metric-collection","text":"","title":"Metric Collection"},{"location":"metrics-and-monitoring/#prometheus-configuration","text":"Configure Prometheus to scrape DataMover metrics: # prometheus.yml scrape_configs: - job_name: 'datamover-operator' static_configs: - targets: ['datamover-operator-controller-manager-metrics-service:8080'] scrape_interval: 30s metrics_path: /metrics","title":"Prometheus Configuration"},{"location":"metrics-and-monitoring/#servicemonitor-prometheus-operator","text":"For Prometheus Operator deployments: apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: datamover-operator namespace: datamover-operator-system spec: selector: matchLabels: control-plane: controller-manager endpoints: - port: https scheme: https path: /metrics tlsConfig: insecureSkipVerify: true","title":"ServiceMonitor (Prometheus Operator)"},{"location":"metrics-and-monitoring/#manual-metric-access","text":"Access metrics directly: # Port-forward to metrics endpoint kubectl port-forward -n datamover-operator-system service/datamover-operator-controller-manager-metrics-service 8080:8080 # Query metrics curl http://localhost:8080/metrics | grep datamover","title":"Manual Metric Access"},{"location":"metrics-and-monitoring/#monitoring-dashboards","text":"","title":"Monitoring Dashboards"},{"location":"metrics-and-monitoring/#grafana-dashboard-example","text":"Create comprehensive Grafana dashboard: { \"dashboard\": { \"title\": \"DataMover Operator\", \"panels\": [ { \"title\": \"Operation Success Rate\", \"type\": \"stat\", \"targets\": [ { \"expr\": \"rate(datamover_operations_total{status=\\\"success\\\"}[5m]) / rate(datamover_operations_total[5m]) * 100\" } ] }, { \"title\": \"Phase Duration\", \"type\": \"graph\", \"targets\": [ { \"expr\": \"histogram_quantile(0.95, rate(datamover_phase_duration_seconds_bucket[5m]))\" } ] }, { \"title\": \"Operations by Phase\", \"type\": \"graph\", \"targets\": [ { \"expr\": \"rate(datamover_operations_total[5m])\" } ] } ] } }","title":"Grafana Dashboard Example"},{"location":"metrics-and-monitoring/#key-performance-indicators-kpis","text":"Monitor these critical metrics: Success Rate : Overall operation success percentage Phase Duration : Time taken for each operation phase Failure Rate : Operations failing per time period Cleanup Success : PVC cleanup operation success rate","title":"Key Performance Indicators (KPIs)"},{"location":"metrics-and-monitoring/#dashboard-panels","text":"","title":"Dashboard Panels"},{"location":"metrics-and-monitoring/#success-rate-panel","text":"# Overall success rate rate(datamover_operations_total{status=\"success\"}[5m]) / rate(datamover_operations_total[5m]) * 100","title":"Success Rate Panel"},{"location":"metrics-and-monitoring/#average-phase-duration-panel","text":"# Average phase duration by phase rate(datamover_phase_duration_seconds_sum[5m]) / rate(datamover_phase_duration_seconds_count[5m])","title":"Average Phase Duration Panel"},{"location":"metrics-and-monitoring/#operations-volume-panel","text":"# Operations per minute by phase rate(datamover_operations_total[1m]) * 60","title":"Operations Volume Panel"},{"location":"metrics-and-monitoring/#cleanup-success-rate-panel","text":"# Cleanup success rate rate(datamover_cleanup_operations_total{status=\"success\"}[5m]) / rate(datamover_cleanup_operations_total[5m]) * 100","title":"Cleanup Success Rate Panel"},{"location":"metrics-and-monitoring/#alerting","text":"","title":"Alerting"},{"location":"metrics-and-monitoring/#prometheus-alerting-rules","text":"Configure alerts for operational issues: groups: - name: datamover.rules rules: # High failure rate alert - alert: DataMoverHighFailureRate expr: | ( rate(datamover_operations_total{status=\"failure\"}[5m]) / rate(datamover_operations_total[5m]) ) > 0.1 for: 2m labels: severity: warning annotations: summary: \"DataMover high failure rate\" description: \"DataMover failure rate is above 10% in namespace {{ $labels.namespace }}\" # Long-running operations alert - alert: DataMoverLongRunningOperation expr: | histogram_quantile(0.95, rate(datamover_phase_duration_seconds_bucket[5m])) > 1800 for: 5m labels: severity: warning annotations: summary: \"DataMover operations taking too long\" description: \"95th percentile of DataMover operations exceeds 30 minutes\" # Cleanup failures alert - alert: DataMoverCleanupFailures expr: | rate(datamover_cleanup_operations_total{status=\"failure\"}[5m]) > 0 for: 1m labels: severity: warning annotations: summary: \"DataMover cleanup operations failing\" description: \"PVC cleanup operations are failing in namespace {{ $labels.namespace }}\" # No operations alert (for scheduled backups) - alert: DataMoverNoOperations expr: | rate(datamover_operations_total[1h]) == 0 for: 2h labels: severity: info annotations: summary: \"No DataMover operations detected\" description: \"No DataMover operations in the last hour\" # Job retry exhaustion alert - alert: DataMoverJobRetriesExhausted expr: | rate(datamover_pod_creation_operations_total{status=\"failure\"}[5m]) > 0 for: 0m labels: severity: critical annotations: summary: \"DataMover jobs exhausting retries\" description: \"DataMover jobs are failing after all retry attempts\"","title":"Prometheus Alerting Rules"},{"location":"metrics-and-monitoring/#alert-severity-levels","text":"Critical : Immediate attention required - Job retries exhausted - Complete operation failures - Security violations Warning : Investigation needed - High failure rates - Performance degradation - Cleanup failures Info : Awareness notifications - No operations detected - Configuration changes - Routine maintenance","title":"Alert Severity Levels"},{"location":"metrics-and-monitoring/#performance-monitoring","text":"","title":"Performance Monitoring"},{"location":"metrics-and-monitoring/#baseline-metrics","text":"Establish baseline performance metrics: Metric Typical Range Alert Threshold PVC Clone Creation 30s - 5min > 10min Job Execution 1min - 30min > 60min Cleanup Operation 5s - 30s > 2min Overall Success Rate > 95% < 90%","title":"Baseline Metrics"},{"location":"metrics-and-monitoring/#performance-queries","text":"","title":"Performance Queries"},{"location":"metrics-and-monitoring/#average-operation-duration","text":"# Average duration by phase over last hour avg_over_time( rate(datamover_phase_duration_seconds_sum[5m])[1h:5m] ) / avg_over_time( rate(datamover_phase_duration_seconds_count[5m])[1h:5m] )","title":"Average Operation Duration"},{"location":"metrics-and-monitoring/#success-rate-trend","text":"# Success rate trend over 24 hours rate(datamover_operations_total{status=\"success\"}[1h]) / rate(datamover_operations_total[1h])","title":"Success Rate Trend"},{"location":"metrics-and-monitoring/#resource-utilization-correlation","text":"# Correlate phase duration with resource usage rate(datamover_phase_duration_seconds_sum[5m]) and on() rate(container_cpu_usage_seconds_total{pod=~\"datamover-operator.*\"}[5m])","title":"Resource Utilization Correlation"},{"location":"pvc-cloning/","text":"PVC Cloning This document explains how PVC (PersistentVolumeClaim) cloning works in the DataMover Operator and the requirements for successful cloning operations. Overview PVC cloning is the foundation of the DataMover Operator. It creates a copy of an existing PVC without requiring volume snapshots, allowing data to be safely backed up while the original application continues running. How PVC Cloning Works Clone Creation Process Source PVC Verification : The operator checks that the source PVC exists and is bound Clone PVC Creation : Creates a new PVC with dataSource pointing to the source PVC Volume Provisioning : The CSI driver creates a new volume as a clone of the source Binding : The cloned PVC becomes bound and ready for use Clone Naming Convention Cloned PVCs follow this naming pattern: <source-pvc-name>-cloned-<timestamp> Example: - Source PVC: web-app-data - Cloned PVC: web-app-data-cloned-20240806143052 Compatible CSI Drivers CSI Driver Clone Support Notes AWS EBS CSI \u2705 Yes Supports cross-AZ cloning GCE PD CSI \u2705 Yes Regional and zonal disks Azure Disk CSI \u2705 Yes Premium and Standard SSDs vSphere CSI \u2705 Yes vSAN and VMFS datastores Ceph CSI \u2705 Yes RBD/CephFS clone feature OpenEBS \u26a0\ufe0f Partial Depends on storage engine Clone Configuration Basic Clone Specification The DataMover operator automatically handles clone creation. You only need to specify the source PVC: apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: backup-web-data spec: sourcePvc: \"web-app-data\" # Source PVC to clone secretName: \"storage-credentials\" Clone Behavior Storage Class : Clones inherit the storage class of the source PVC Access Modes : Clones inherit access modes from the source PVC Size : Clones have the same size as the source PVC Labels : Clones get operator-specific labels for tracking Clone Lifecycle Management Automatic Cleanup When deletePvcAfterBackup is enabled, clones are automatically deleted after successful backup: spec: sourcePvc: \"web-app-data\" secretName: \"storage-credentials\" deletePvcAfterBackup: true # Auto-delete clone after backup Manual Cleanup If automatic cleanup is disabled, you can manually clean up clones: # List clones created by the operator kubectl get pvc -l app.kubernetes.io/created-by=datamover-operator # Delete specific clone kubectl delete pvc restored-web-app-data-20240806143052 # Delete all operator-created clones kubectl delete pvc -l app.kubernetes.io/created-by=datamover-operator Performance Considerations Clone Creation Time Clone creation time depends on: Storage Backend : NVMe SSDs are faster than HDDs Volume Size : Larger volumes take longer to clone CSI Driver Implementation : Some drivers use copy-on-write, others full copy Network Latency : For network-attached storage Resource Usage During cloning: Storage : Temporary storage equal to source PVC size I/O : Minimal impact on source PVC performance Network : Bandwidth usage depends on storage backend Best Practices Schedule During Low Traffic : Clone during maintenance windows when possible Monitor Storage Usage : Ensure sufficient storage for clones Use Fast Storage Classes : Clone to high-performance storage when available Enable Auto-Cleanup : Reduce storage waste with automatic cleanup Troubleshooting Clone Issues Common Problems 1. Clone Creation Fails Error : Failed to create cloned PVC Possible Causes : - CSI driver doesn't support cloning - Insufficient storage quota - Source PVC is not bound - Storage class restrictions Diagnosis : # Check source PVC status kubectl get pvc <source-pvc-name> # Check storage class kubectl describe storageclass <storage-class-name> # Check CSI driver capabilities kubectl describe csidriver <csi-driver-name> 2. Clone Stuck in Pending Error : Clone PVC remains in Pending state Possible Causes : - No available storage - Node affinity conflicts - CSI driver issues Diagnosis : # Check clone PVC events kubectl describe pvc <clone-pvc-name> # Check storage capacity kubectl get pv # Check node availability kubectl get nodes 3. Clone Performance Issues Error : Clone creation is very slow Solutions : - Use faster storage classes - Check storage backend performance - Verify network connectivity - Consider off-peak scheduling Debug Commands # Monitor clone creation kubectl get pvc -w # Check detailed clone events kubectl describe pvc <clone-pvc-name> # View operator logs for clone operations kubectl logs -n datamover-operator-system deployment/datamover-operator-controller-manager | grep -i clone # Check storage class details kubectl describe storageclass <storage-class> Advanced Clone Scenarios Cross-Namespace Cloning Clones are created in the same namespace as the DataMover resource: apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: cross-ns-backup namespace: backup-namespace # Clone created here spec: sourcePvc: \"app-data\" # Must exist in backup-namespace secretName: \"storage-credentials\" Multiple Clones from Same Source You can create multiple DataMover resources from the same source PVC: # Development backup apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: dev-backup spec: sourcePvc: \"shared-data\" secretName: \"dev-storage\" --- # Production backup apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: prod-backup spec: sourcePvc: \"shared-data\" secretName: \"prod-storage\" Security Considerations Access Control Clone creation requires PVC creation permissions Clones inherit security context from operator Ensure proper RBAC for clone management Data Security Clones contain exact copy of source data Apply same security policies as source PVC Consider encryption for sensitive data Cleanup Security Ensure proper deletion of clones Verify data scrubbing policies Monitor clone lifecycle for compliance Monitoring Clone Operations Metrics The operator provides metrics for clone operations: datamover_operations_total{phase=\"CreatingClonedPVC\"} datamover_phase_duration_seconds{phase=\"CreatingClonedPVC\"} Logging Monitor clone operations through operator logs: kubectl logs -n datamover-operator-system deployment/datamover-operator-controller-manager | grep -i \"clone\\|pvc\" Status Tracking Check clone status through DataMover resource: kubectl get datamover -o wide kubectl describe datamover <datamover-name>","title":"PVC Cloning"},{"location":"pvc-cloning/#pvc-cloning","text":"This document explains how PVC (PersistentVolumeClaim) cloning works in the DataMover Operator and the requirements for successful cloning operations.","title":"PVC Cloning"},{"location":"pvc-cloning/#overview","text":"PVC cloning is the foundation of the DataMover Operator. It creates a copy of an existing PVC without requiring volume snapshots, allowing data to be safely backed up while the original application continues running.","title":"Overview"},{"location":"pvc-cloning/#how-pvc-cloning-works","text":"","title":"How PVC Cloning Works"},{"location":"pvc-cloning/#clone-creation-process","text":"Source PVC Verification : The operator checks that the source PVC exists and is bound Clone PVC Creation : Creates a new PVC with dataSource pointing to the source PVC Volume Provisioning : The CSI driver creates a new volume as a clone of the source Binding : The cloned PVC becomes bound and ready for use","title":"Clone Creation Process"},{"location":"pvc-cloning/#clone-naming-convention","text":"Cloned PVCs follow this naming pattern: <source-pvc-name>-cloned-<timestamp> Example: - Source PVC: web-app-data - Cloned PVC: web-app-data-cloned-20240806143052","title":"Clone Naming Convention"},{"location":"pvc-cloning/#compatible-csi-drivers","text":"CSI Driver Clone Support Notes AWS EBS CSI \u2705 Yes Supports cross-AZ cloning GCE PD CSI \u2705 Yes Regional and zonal disks Azure Disk CSI \u2705 Yes Premium and Standard SSDs vSphere CSI \u2705 Yes vSAN and VMFS datastores Ceph CSI \u2705 Yes RBD/CephFS clone feature OpenEBS \u26a0\ufe0f Partial Depends on storage engine","title":"Compatible CSI Drivers"},{"location":"pvc-cloning/#clone-configuration","text":"","title":"Clone Configuration"},{"location":"pvc-cloning/#basic-clone-specification","text":"The DataMover operator automatically handles clone creation. You only need to specify the source PVC: apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: backup-web-data spec: sourcePvc: \"web-app-data\" # Source PVC to clone secretName: \"storage-credentials\"","title":"Basic Clone Specification"},{"location":"pvc-cloning/#clone-behavior","text":"Storage Class : Clones inherit the storage class of the source PVC Access Modes : Clones inherit access modes from the source PVC Size : Clones have the same size as the source PVC Labels : Clones get operator-specific labels for tracking","title":"Clone Behavior"},{"location":"pvc-cloning/#clone-lifecycle-management","text":"","title":"Clone Lifecycle Management"},{"location":"pvc-cloning/#automatic-cleanup","text":"When deletePvcAfterBackup is enabled, clones are automatically deleted after successful backup: spec: sourcePvc: \"web-app-data\" secretName: \"storage-credentials\" deletePvcAfterBackup: true # Auto-delete clone after backup","title":"Automatic Cleanup"},{"location":"pvc-cloning/#manual-cleanup","text":"If automatic cleanup is disabled, you can manually clean up clones: # List clones created by the operator kubectl get pvc -l app.kubernetes.io/created-by=datamover-operator # Delete specific clone kubectl delete pvc restored-web-app-data-20240806143052 # Delete all operator-created clones kubectl delete pvc -l app.kubernetes.io/created-by=datamover-operator","title":"Manual Cleanup"},{"location":"pvc-cloning/#performance-considerations","text":"","title":"Performance Considerations"},{"location":"pvc-cloning/#clone-creation-time","text":"Clone creation time depends on: Storage Backend : NVMe SSDs are faster than HDDs Volume Size : Larger volumes take longer to clone CSI Driver Implementation : Some drivers use copy-on-write, others full copy Network Latency : For network-attached storage","title":"Clone Creation Time"},{"location":"pvc-cloning/#resource-usage","text":"During cloning: Storage : Temporary storage equal to source PVC size I/O : Minimal impact on source PVC performance Network : Bandwidth usage depends on storage backend","title":"Resource Usage"},{"location":"pvc-cloning/#best-practices","text":"Schedule During Low Traffic : Clone during maintenance windows when possible Monitor Storage Usage : Ensure sufficient storage for clones Use Fast Storage Classes : Clone to high-performance storage when available Enable Auto-Cleanup : Reduce storage waste with automatic cleanup","title":"Best Practices"},{"location":"pvc-cloning/#troubleshooting-clone-issues","text":"","title":"Troubleshooting Clone Issues"},{"location":"pvc-cloning/#common-problems","text":"","title":"Common Problems"},{"location":"pvc-cloning/#1-clone-creation-fails","text":"Error : Failed to create cloned PVC Possible Causes : - CSI driver doesn't support cloning - Insufficient storage quota - Source PVC is not bound - Storage class restrictions Diagnosis : # Check source PVC status kubectl get pvc <source-pvc-name> # Check storage class kubectl describe storageclass <storage-class-name> # Check CSI driver capabilities kubectl describe csidriver <csi-driver-name>","title":"1. Clone Creation Fails"},{"location":"pvc-cloning/#2-clone-stuck-in-pending","text":"Error : Clone PVC remains in Pending state Possible Causes : - No available storage - Node affinity conflicts - CSI driver issues Diagnosis : # Check clone PVC events kubectl describe pvc <clone-pvc-name> # Check storage capacity kubectl get pv # Check node availability kubectl get nodes","title":"2. Clone Stuck in Pending"},{"location":"pvc-cloning/#3-clone-performance-issues","text":"Error : Clone creation is very slow Solutions : - Use faster storage classes - Check storage backend performance - Verify network connectivity - Consider off-peak scheduling","title":"3. Clone Performance Issues"},{"location":"pvc-cloning/#debug-commands","text":"# Monitor clone creation kubectl get pvc -w # Check detailed clone events kubectl describe pvc <clone-pvc-name> # View operator logs for clone operations kubectl logs -n datamover-operator-system deployment/datamover-operator-controller-manager | grep -i clone # Check storage class details kubectl describe storageclass <storage-class>","title":"Debug Commands"},{"location":"pvc-cloning/#advanced-clone-scenarios","text":"","title":"Advanced Clone Scenarios"},{"location":"pvc-cloning/#cross-namespace-cloning","text":"Clones are created in the same namespace as the DataMover resource: apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: cross-ns-backup namespace: backup-namespace # Clone created here spec: sourcePvc: \"app-data\" # Must exist in backup-namespace secretName: \"storage-credentials\"","title":"Cross-Namespace Cloning"},{"location":"pvc-cloning/#multiple-clones-from-same-source","text":"You can create multiple DataMover resources from the same source PVC: # Development backup apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: dev-backup spec: sourcePvc: \"shared-data\" secretName: \"dev-storage\" --- # Production backup apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: prod-backup spec: sourcePvc: \"shared-data\" secretName: \"prod-storage\"","title":"Multiple Clones from Same Source"},{"location":"pvc-cloning/#security-considerations","text":"","title":"Security Considerations"},{"location":"pvc-cloning/#access-control","text":"Clone creation requires PVC creation permissions Clones inherit security context from operator Ensure proper RBAC for clone management","title":"Access Control"},{"location":"pvc-cloning/#data-security","text":"Clones contain exact copy of source data Apply same security policies as source PVC Consider encryption for sensitive data","title":"Data Security"},{"location":"pvc-cloning/#cleanup-security","text":"Ensure proper deletion of clones Verify data scrubbing policies Monitor clone lifecycle for compliance","title":"Cleanup Security"},{"location":"pvc-cloning/#monitoring-clone-operations","text":"","title":"Monitoring Clone Operations"},{"location":"pvc-cloning/#metrics","text":"The operator provides metrics for clone operations: datamover_operations_total{phase=\"CreatingClonedPVC\"} datamover_phase_duration_seconds{phase=\"CreatingClonedPVC\"}","title":"Metrics"},{"location":"pvc-cloning/#logging","text":"Monitor clone operations through operator logs: kubectl logs -n datamover-operator-system deployment/datamover-operator-controller-manager | grep -i \"clone\\|pvc\"","title":"Logging"},{"location":"pvc-cloning/#status-tracking","text":"Check clone status through DataMover resource: kubectl get datamover -o wide kubectl describe datamover <datamover-name>","title":"Status Tracking"},{"location":"timestamp-organization/","text":"Timestamp Organization This document explains the timestamp organization feature that allows for structured, time-based backup organization in remote storage. Overview The timestamp organization feature creates timestamped folders in remote storage, enabling point-in-time backups, easier data management, and organized storage structures. When enabled, each backup operation creates a unique timestamp-prefixed directory. Feature Configuration Enabling Timestamp Organization Enable timestamp prefixes in your DataMover specification: apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: timestamped-backup spec: sourcePvc: \"app-data\" secretName: \"storage-credentials\" addTimestampPrefix: true # Enable timestamp organization Disabling Timestamp Organization For direct synchronization to bucket root: apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: direct-sync spec: sourcePvc: \"app-data\" secretName: \"storage-credentials\" addTimestampPrefix: false # Sync directly to bucket root Default Behavior : addTimestampPrefix: false Timestamp Format Format Specification Timestamps use the format: YYYY-MM-DD-HHMMSS YYYY : 4-digit year MM : 2-digit month (01-12) DD : 2-digit day (01-31) HH : 2-digit hour (00-23) MM : 2-digit minute (00-59) SS : 2-digit second (00-59) Examples 2024-08-06-143052 # August 6, 2024, 14:30:52 2024-12-25-091500 # December 25, 2024, 09:15:00 2025-01-01-000000 # January 1, 2025, 00:00:00 Timezone Considerations Timestamps are generated in UTC timezone to ensure consistency across different operator deployments and geographical locations. Storage Structure With Timestamp Organization When addTimestampPrefix: true : my-backup-bucket/ \u251c\u2500\u2500 2024-08-06-143052/ # First backup \u2502 \u251c\u2500\u2500 app/ \u2502 \u2502 \u251c\u2500\u2500 config.yaml \u2502 \u2502 \u2514\u2500\u2500 app.log \u2502 \u251c\u2500\u2500 data/ \u2502 \u2502 \u251c\u2500\u2500 database.db \u2502 \u2502 \u2514\u2500\u2500 cache/ \u2502 \u2514\u2500\u2500 logs/ \u2502 \u2514\u2500\u2500 application.log \u251c\u2500\u2500 2024-08-06-151225/ # Second backup \u2502 \u251c\u2500\u2500 app/ \u2502 \u2502 \u251c\u2500\u2500 config.yaml \u2502 \u2502 \u2514\u2500\u2500 app.log \u2502 \u251c\u2500\u2500 data/ \u2502 \u2502 \u251c\u2500\u2500 database.db \u2502 \u2502 \u2514\u2500\u2500 cache/ \u2502 \u2514\u2500\u2500 logs/ \u2502 \u2514\u2500\u2500 application.log \u2514\u2500\u2500 2024-08-06-163408/ # Third backup \u251c\u2500\u2500 app/ \u251c\u2500\u2500 data/ \u2514\u2500\u2500 logs/ Without Timestamp Organization When addTimestampPrefix: false : my-backup-bucket/ \u251c\u2500\u2500 app/ \u2502 \u251c\u2500\u2500 config.yaml \u2502 \u2514\u2500\u2500 app.log \u251c\u2500\u2500 data/ \u2502 \u251c\u2500\u2500 database.db \u2502 \u2514\u2500\u2500 cache/ \u2514\u2500\u2500 logs/ \u2514\u2500\u2500 application.log Use Cases 1. Point-in-Time Recovery Maintain multiple backup versions for recovery: # Daily backup with timestamps apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: daily-backup spec: sourcePvc: \"production-data\" secretName: \"s3-credentials\" addTimestampPrefix: true Benefits : - Recovery to specific time points - Compare data between different backups - Rollback to previous known-good states 2. Compliance and Auditing Maintain historical records for compliance: # Compliance backup with retention apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: compliance-backup spec: sourcePvc: \"financial-data\" secretName: \"secure-storage\" addTimestampPrefix: true Benefits : - Immutable backup history - Audit trail of data changes - Regulatory compliance support 3. Development Workflows Snapshot development environments: # Development snapshot apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: dev-snapshot spec: sourcePvc: \"dev-workspace\" secretName: \"dev-storage\" addTimestampPrefix: true Benefits : - Environment versioning - Feature branch data isolation - Easy environment restoration 4. Continuous Synchronization For live synchronization without versioning: # Live sync without timestamps apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: live-sync spec: sourcePvc: \"shared-data\" secretName: \"sync-storage\" addTimestampPrefix: false Benefits : - Real-time data replication - Mirror maintenance - Disaster recovery preparation Implementation Details Environment Variable The feature is controlled by the ADD_TIMESTAMP_PREFIX environment variable passed to the rclone container: # When addTimestampPrefix: true ADD_TIMESTAMP_PREFIX=true # When addTimestampPrefix: false ADD_TIMESTAMP_PREFIX=false Rclone Integration The custom entrypoint script ( entrypoint.sh ) handles timestamp generation: #!/bin/bash if [ \"$ADD_TIMESTAMP_PREFIX\" = \"true\" ]; then # Generate timestamp in UTC TIMESTAMP=$(date -u +\"%Y-%m-%d-%H%M%S\") DESTINATION=\"s3:${BUCKET_NAME}/${TIMESTAMP}/\" else DESTINATION=\"s3:${BUCKET_NAME}/\" fi # Execute rclone sync rclone sync /data/ \"$DESTINATION\" --progress Timestamp Generation Timestamps are generated at job execution time, ensuring: Uniqueness : Each backup gets a unique timestamp Consistency : UTC timezone for global consistency Precision : Second-level precision for frequent backups Management and Cleanup Listing Timestamped Backups Use rclone or cloud provider tools to list backups: # List all timestamped backups rclone lsd s3:my-bucket/ # List backups for specific date rclone lsd s3:my-bucket/ | grep \"2024-08-06\" Automated Cleanup Implement cleanup policies using cloud provider lifecycle rules: AWS S3 Lifecycle Policy { \"Rules\": [ { \"ID\": \"DataMoverBackupRetention\", \"Status\": \"Enabled\", \"Filter\": { \"Prefix\": \"2025-\" }, \"Expiration\": { \"Days\": 30 } } ] } Manual Cleanup Script #!/bin/bash # Cleanup backups older than 30 days BUCKET=\"my-backup-bucket\" CUTOFF_DATE=$(date -d \"30 days ago\" +\"%Y-%m-%d\") # List and delete old backups rclone lsd s3:$BUCKET/ | while read line; do BACKUP_DATE=$(echo $line | awk '{print $5}' | cut -d'-' -f1-3) if [[ \"$BACKUP_DATE\" < \"$CUTOFF_DATE\" ]]; then BACKUP_PATH=$(echo $line | awk '{print $5}') echo \"Deleting old backup: $BACKUP_PATH\" rclone purge s3:$BUCKET/$BACKUP_PATH/ fi done Storage Cost Optimization Optimize storage costs with timestamped backups: Lifecycle Policies : Move old backups to cheaper storage tiers Compression : Enable compression for text-heavy data Retention Policies : Implement appropriate retention periods","title":"Timestamp Organization"},{"location":"timestamp-organization/#timestamp-organization","text":"This document explains the timestamp organization feature that allows for structured, time-based backup organization in remote storage.","title":"Timestamp Organization"},{"location":"timestamp-organization/#overview","text":"The timestamp organization feature creates timestamped folders in remote storage, enabling point-in-time backups, easier data management, and organized storage structures. When enabled, each backup operation creates a unique timestamp-prefixed directory.","title":"Overview"},{"location":"timestamp-organization/#feature-configuration","text":"","title":"Feature Configuration"},{"location":"timestamp-organization/#enabling-timestamp-organization","text":"Enable timestamp prefixes in your DataMover specification: apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: timestamped-backup spec: sourcePvc: \"app-data\" secretName: \"storage-credentials\" addTimestampPrefix: true # Enable timestamp organization","title":"Enabling Timestamp Organization"},{"location":"timestamp-organization/#disabling-timestamp-organization","text":"For direct synchronization to bucket root: apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: direct-sync spec: sourcePvc: \"app-data\" secretName: \"storage-credentials\" addTimestampPrefix: false # Sync directly to bucket root Default Behavior : addTimestampPrefix: false","title":"Disabling Timestamp Organization"},{"location":"timestamp-organization/#timestamp-format","text":"","title":"Timestamp Format"},{"location":"timestamp-organization/#format-specification","text":"Timestamps use the format: YYYY-MM-DD-HHMMSS YYYY : 4-digit year MM : 2-digit month (01-12) DD : 2-digit day (01-31) HH : 2-digit hour (00-23) MM : 2-digit minute (00-59) SS : 2-digit second (00-59)","title":"Format Specification"},{"location":"timestamp-organization/#examples","text":"2024-08-06-143052 # August 6, 2024, 14:30:52 2024-12-25-091500 # December 25, 2024, 09:15:00 2025-01-01-000000 # January 1, 2025, 00:00:00","title":"Examples"},{"location":"timestamp-organization/#timezone-considerations","text":"Timestamps are generated in UTC timezone to ensure consistency across different operator deployments and geographical locations.","title":"Timezone Considerations"},{"location":"timestamp-organization/#storage-structure","text":"","title":"Storage Structure"},{"location":"timestamp-organization/#with-timestamp-organization","text":"When addTimestampPrefix: true : my-backup-bucket/ \u251c\u2500\u2500 2024-08-06-143052/ # First backup \u2502 \u251c\u2500\u2500 app/ \u2502 \u2502 \u251c\u2500\u2500 config.yaml \u2502 \u2502 \u2514\u2500\u2500 app.log \u2502 \u251c\u2500\u2500 data/ \u2502 \u2502 \u251c\u2500\u2500 database.db \u2502 \u2502 \u2514\u2500\u2500 cache/ \u2502 \u2514\u2500\u2500 logs/ \u2502 \u2514\u2500\u2500 application.log \u251c\u2500\u2500 2024-08-06-151225/ # Second backup \u2502 \u251c\u2500\u2500 app/ \u2502 \u2502 \u251c\u2500\u2500 config.yaml \u2502 \u2502 \u2514\u2500\u2500 app.log \u2502 \u251c\u2500\u2500 data/ \u2502 \u2502 \u251c\u2500\u2500 database.db \u2502 \u2502 \u2514\u2500\u2500 cache/ \u2502 \u2514\u2500\u2500 logs/ \u2502 \u2514\u2500\u2500 application.log \u2514\u2500\u2500 2024-08-06-163408/ # Third backup \u251c\u2500\u2500 app/ \u251c\u2500\u2500 data/ \u2514\u2500\u2500 logs/","title":"With Timestamp Organization"},{"location":"timestamp-organization/#without-timestamp-organization","text":"When addTimestampPrefix: false : my-backup-bucket/ \u251c\u2500\u2500 app/ \u2502 \u251c\u2500\u2500 config.yaml \u2502 \u2514\u2500\u2500 app.log \u251c\u2500\u2500 data/ \u2502 \u251c\u2500\u2500 database.db \u2502 \u2514\u2500\u2500 cache/ \u2514\u2500\u2500 logs/ \u2514\u2500\u2500 application.log","title":"Without Timestamp Organization"},{"location":"timestamp-organization/#use-cases","text":"","title":"Use Cases"},{"location":"timestamp-organization/#1-point-in-time-recovery","text":"Maintain multiple backup versions for recovery: # Daily backup with timestamps apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: daily-backup spec: sourcePvc: \"production-data\" secretName: \"s3-credentials\" addTimestampPrefix: true Benefits : - Recovery to specific time points - Compare data between different backups - Rollback to previous known-good states","title":"1. Point-in-Time Recovery"},{"location":"timestamp-organization/#2-compliance-and-auditing","text":"Maintain historical records for compliance: # Compliance backup with retention apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: compliance-backup spec: sourcePvc: \"financial-data\" secretName: \"secure-storage\" addTimestampPrefix: true Benefits : - Immutable backup history - Audit trail of data changes - Regulatory compliance support","title":"2. Compliance and Auditing"},{"location":"timestamp-organization/#3-development-workflows","text":"Snapshot development environments: # Development snapshot apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: dev-snapshot spec: sourcePvc: \"dev-workspace\" secretName: \"dev-storage\" addTimestampPrefix: true Benefits : - Environment versioning - Feature branch data isolation - Easy environment restoration","title":"3. Development Workflows"},{"location":"timestamp-organization/#4-continuous-synchronization","text":"For live synchronization without versioning: # Live sync without timestamps apiVersion: datamover.a-cup-of.coffee/v1alpha1 kind: DataMover metadata: name: live-sync spec: sourcePvc: \"shared-data\" secretName: \"sync-storage\" addTimestampPrefix: false Benefits : - Real-time data replication - Mirror maintenance - Disaster recovery preparation","title":"4. Continuous Synchronization"},{"location":"timestamp-organization/#implementation-details","text":"","title":"Implementation Details"},{"location":"timestamp-organization/#environment-variable","text":"The feature is controlled by the ADD_TIMESTAMP_PREFIX environment variable passed to the rclone container: # When addTimestampPrefix: true ADD_TIMESTAMP_PREFIX=true # When addTimestampPrefix: false ADD_TIMESTAMP_PREFIX=false","title":"Environment Variable"},{"location":"timestamp-organization/#rclone-integration","text":"The custom entrypoint script ( entrypoint.sh ) handles timestamp generation: #!/bin/bash if [ \"$ADD_TIMESTAMP_PREFIX\" = \"true\" ]; then # Generate timestamp in UTC TIMESTAMP=$(date -u +\"%Y-%m-%d-%H%M%S\") DESTINATION=\"s3:${BUCKET_NAME}/${TIMESTAMP}/\" else DESTINATION=\"s3:${BUCKET_NAME}/\" fi # Execute rclone sync rclone sync /data/ \"$DESTINATION\" --progress","title":"Rclone Integration"},{"location":"timestamp-organization/#timestamp-generation","text":"Timestamps are generated at job execution time, ensuring: Uniqueness : Each backup gets a unique timestamp Consistency : UTC timezone for global consistency Precision : Second-level precision for frequent backups","title":"Timestamp Generation"},{"location":"timestamp-organization/#management-and-cleanup","text":"","title":"Management and Cleanup"},{"location":"timestamp-organization/#listing-timestamped-backups","text":"Use rclone or cloud provider tools to list backups: # List all timestamped backups rclone lsd s3:my-bucket/ # List backups for specific date rclone lsd s3:my-bucket/ | grep \"2024-08-06\"","title":"Listing Timestamped Backups"},{"location":"timestamp-organization/#automated-cleanup","text":"Implement cleanup policies using cloud provider lifecycle rules:","title":"Automated Cleanup"},{"location":"timestamp-organization/#aws-s3-lifecycle-policy","text":"{ \"Rules\": [ { \"ID\": \"DataMoverBackupRetention\", \"Status\": \"Enabled\", \"Filter\": { \"Prefix\": \"2025-\" }, \"Expiration\": { \"Days\": 30 } } ] }","title":"AWS S3 Lifecycle Policy"},{"location":"timestamp-organization/#manual-cleanup-script","text":"#!/bin/bash # Cleanup backups older than 30 days BUCKET=\"my-backup-bucket\" CUTOFF_DATE=$(date -d \"30 days ago\" +\"%Y-%m-%d\") # List and delete old backups rclone lsd s3:$BUCKET/ | while read line; do BACKUP_DATE=$(echo $line | awk '{print $5}' | cut -d'-' -f1-3) if [[ \"$BACKUP_DATE\" < \"$CUTOFF_DATE\" ]]; then BACKUP_PATH=$(echo $line | awk '{print $5}') echo \"Deleting old backup: $BACKUP_PATH\" rclone purge s3:$BUCKET/$BACKUP_PATH/ fi done","title":"Manual Cleanup Script"},{"location":"timestamp-organization/#storage-cost-optimization","text":"Optimize storage costs with timestamped backups: Lifecycle Policies : Move old backups to cheaper storage tiers Compression : Enable compression for text-heavy data Retention Policies : Implement appropriate retention periods","title":"Storage Cost Optimization"}]}